2026-01-30 08:13:21 | INFO | Starting GNN model training
2026-01-30 08:13:21 | INFO | Device: cpu
2026-01-30 08:13:22 | INFO | Train batches: 1169
2026-01-30 08:13:22 | INFO | Model parameters: 49,283
2026-01-30 08:13:23 | INFO | Epoch 1 | Batch 0/1169 | Loss: 0.0711
2026-01-30 08:13:25 | INFO | Epoch 1 | Batch 100/1169 | Loss: 0.0634
2026-01-30 08:13:26 | INFO | Epoch 1 | Batch 200/1169 | Loss: 0.0616
2026-01-30 08:13:27 | INFO | Epoch 1 | Batch 300/1169 | Loss: 0.0615
2026-01-30 08:13:28 | INFO | Epoch 1 | Batch 400/1169 | Loss: 0.0587
2026-01-30 08:13:30 | INFO | Epoch 1 | Batch 500/1169 | Loss: 0.0654
2026-01-30 08:13:31 | INFO | Epoch 1 | Batch 600/1169 | Loss: 0.0624
2026-01-30 08:13:32 | INFO | Epoch 1 | Batch 700/1169 | Loss: 0.0613
2026-01-30 08:13:33 | INFO | Epoch 1 | Batch 800/1169 | Loss: 0.0577
2026-01-30 08:13:35 | INFO | Epoch 1 | Batch 900/1169 | Loss: 0.0600
2026-01-30 08:13:36 | INFO | Epoch 1 | Batch 1000/1169 | Loss: 0.0635
2026-01-30 08:13:37 | INFO | Epoch 1 | Batch 1100/1169 | Loss: 0.0621
2026-01-30 08:13:40 | INFO | Epoch 1 | Train Loss: 0.0621 | Val Loss: 0.0482
2026-01-30 08:13:40 | INFO | Val F1: 0.1216 | Prec: 0.0663 | Rec: 0.7242 | AUC: 0.7513
2026-01-30 08:13:40 | INFO | New best model saved (F1: 0.1216)
2026-01-30 08:13:40 | INFO | Epoch 2 | Batch 0/1169 | Loss: 0.0611
2026-01-30 08:13:41 | INFO | Epoch 2 | Batch 100/1169 | Loss: 0.0594
2026-01-30 08:13:42 | INFO | Epoch 2 | Batch 200/1169 | Loss: 0.0602
2026-01-30 08:13:44 | INFO | Epoch 2 | Batch 300/1169 | Loss: 0.0592
2026-01-30 08:13:45 | INFO | Epoch 2 | Batch 400/1169 | Loss: 0.0614
2026-01-30 08:13:46 | INFO | Epoch 2 | Batch 500/1169 | Loss: 0.0616
2026-01-30 08:13:48 | INFO | Epoch 2 | Batch 600/1169 | Loss: 0.0611
2026-01-30 08:13:49 | INFO | Epoch 2 | Batch 700/1169 | Loss: 0.0577
2026-01-30 08:13:50 | INFO | Epoch 2 | Batch 800/1169 | Loss: 0.0592
2026-01-30 08:13:51 | INFO | Epoch 2 | Batch 900/1169 | Loss: 0.0577
2026-01-30 08:13:53 | INFO | Epoch 2 | Batch 1000/1169 | Loss: 0.0554
2026-01-30 08:13:54 | INFO | Epoch 2 | Batch 1100/1169 | Loss: 0.0590
2026-01-30 08:13:56 | INFO | Epoch 2 | Train Loss: 0.0598 | Val Loss: 0.0437
2026-01-30 08:13:56 | INFO | Val F1: 0.1367 | Prec: 0.0758 | Rec: 0.6945 | AUC: 0.7615
2026-01-30 08:13:56 | INFO | New best model saved (F1: 0.1367)
2026-01-30 08:13:56 | INFO | Epoch 3 | Batch 0/1169 | Loss: 0.0604
2026-01-30 08:13:57 | INFO | Epoch 3 | Batch 100/1169 | Loss: 0.0598
2026-01-30 08:13:59 | INFO | Epoch 3 | Batch 200/1169 | Loss: 0.0564
2026-01-30 08:14:00 | INFO | Epoch 3 | Batch 300/1169 | Loss: 0.0543
2026-01-30 08:14:01 | INFO | Epoch 3 | Batch 400/1169 | Loss: 0.0560
2026-01-30 08:14:02 | INFO | Epoch 3 | Batch 500/1169 | Loss: 0.0570
2026-01-30 08:14:04 | INFO | Epoch 3 | Batch 600/1169 | Loss: 0.0566
2026-01-30 08:14:05 | INFO | Epoch 3 | Batch 700/1169 | Loss: 0.0553
2026-01-30 08:14:06 | INFO | Epoch 3 | Batch 800/1169 | Loss: 0.0578
2026-01-30 08:14:07 | INFO | Epoch 3 | Batch 900/1169 | Loss: 0.0583
2026-01-30 08:14:08 | INFO | Epoch 3 | Batch 1000/1169 | Loss: 0.0584
2026-01-30 08:14:09 | INFO | Epoch 3 | Batch 1100/1169 | Loss: 0.0579
2026-01-30 08:14:11 | INFO | Epoch 3 | Train Loss: 0.0586 | Val Loss: 0.0451
2026-01-30 08:14:11 | INFO | Val F1: 0.1365 | Prec: 0.0754 | Rec: 0.7165 | AUC: 0.7668
2026-01-30 08:14:11 | INFO | Epoch 4 | Batch 0/1169 | Loss: 0.0567
2026-01-30 08:14:12 | INFO | Epoch 4 | Batch 100/1169 | Loss: 0.0564
2026-01-30 08:14:13 | INFO | Epoch 4 | Batch 200/1169 | Loss: 0.0600
2026-01-30 08:14:14 | INFO | Epoch 4 | Batch 300/1169 | Loss: 0.0585
2026-01-30 08:14:15 | INFO | Epoch 4 | Batch 400/1169 | Loss: 0.0564
2026-01-30 08:14:17 | INFO | Epoch 4 | Batch 500/1169 | Loss: 0.0598
2026-01-30 08:14:18 | INFO | Epoch 4 | Batch 600/1169 | Loss: 0.0596
2026-01-30 08:14:19 | INFO | Epoch 4 | Batch 700/1169 | Loss: 0.0606
2026-01-30 08:14:20 | INFO | Epoch 4 | Batch 800/1169 | Loss: 0.0548
2026-01-30 08:14:21 | INFO | Epoch 4 | Batch 900/1169 | Loss: 0.0548
2026-01-30 08:14:23 | INFO | Epoch 4 | Batch 1000/1169 | Loss: 0.0598
2026-01-30 08:14:24 | INFO | Epoch 4 | Batch 1100/1169 | Loss: 0.0558
2026-01-30 08:14:26 | INFO | Epoch 4 | Train Loss: 0.0577 | Val Loss: 0.0426
2026-01-30 08:14:26 | INFO | Val F1: 0.1437 | Prec: 0.0801 | Rec: 0.7000 | AUC: 0.7725
2026-01-30 08:14:26 | INFO | New best model saved (F1: 0.1437)
2026-01-30 08:14:26 | INFO | Epoch 5 | Batch 0/1169 | Loss: 0.0549
2026-01-30 08:14:27 | INFO | Epoch 5 | Batch 100/1169 | Loss: 0.0573
2026-01-30 08:14:29 | INFO | Epoch 5 | Batch 200/1169 | Loss: 0.0585
2026-01-30 08:14:30 | INFO | Epoch 5 | Batch 300/1169 | Loss: 0.0592
2026-01-30 08:14:31 | INFO | Epoch 5 | Batch 400/1169 | Loss: 0.0585
2026-01-30 08:14:32 | INFO | Epoch 5 | Batch 500/1169 | Loss: 0.0566
2026-01-30 08:14:34 | INFO | Epoch 5 | Batch 600/1169 | Loss: 0.0550
2026-01-30 08:14:35 | INFO | Epoch 5 | Batch 700/1169 | Loss: 0.0582
2026-01-30 08:14:36 | INFO | Epoch 5 | Batch 800/1169 | Loss: 0.0589
2026-01-30 08:14:37 | INFO | Epoch 5 | Batch 900/1169 | Loss: 0.0531
2026-01-30 08:14:38 | INFO | Epoch 5 | Batch 1000/1169 | Loss: 0.0535
2026-01-30 08:14:40 | INFO | Epoch 5 | Batch 1100/1169 | Loss: 0.0584
2026-01-30 08:14:42 | INFO | Epoch 5 | Train Loss: 0.0569 | Val Loss: 0.0436
2026-01-30 08:14:42 | INFO | Val F1: 0.1366 | Prec: 0.0753 | Rec: 0.7326 | AUC: 0.7749
2026-01-30 08:14:42 | INFO | Epoch 6 | Batch 0/1169 | Loss: 0.0543
2026-01-30 08:14:43 | INFO | Epoch 6 | Batch 100/1169 | Loss: 0.0580
2026-01-30 08:14:44 | INFO | Epoch 6 | Batch 200/1169 | Loss: 0.0583
2026-01-30 08:14:45 | INFO | Epoch 6 | Batch 300/1169 | Loss: 0.0566
2026-01-30 08:14:46 | INFO | Epoch 6 | Batch 400/1169 | Loss: 0.0552
2026-01-30 08:14:48 | INFO | Epoch 6 | Batch 500/1169 | Loss: 0.0567
2026-01-30 08:14:49 | INFO | Epoch 6 | Batch 600/1169 | Loss: 0.0553
2026-01-30 08:14:51 | INFO | Epoch 6 | Batch 700/1169 | Loss: 0.0606
2026-01-30 08:14:52 | INFO | Epoch 6 | Batch 800/1169 | Loss: 0.0564
2026-01-30 08:14:53 | INFO | Epoch 6 | Batch 900/1169 | Loss: 0.0541
2026-01-30 08:14:55 | INFO | Epoch 6 | Batch 1000/1169 | Loss: 0.0587
2026-01-30 08:14:56 | INFO | Epoch 6 | Batch 1100/1169 | Loss: 0.0567
2026-01-30 08:14:58 | INFO | Epoch 6 | Train Loss: 0.0564 | Val Loss: 0.0411
2026-01-30 08:14:58 | INFO | Val F1: 0.1422 | Prec: 0.0789 | Rec: 0.7171 | AUC: 0.7777
2026-01-30 08:14:58 | INFO | Epoch 7 | Batch 0/1169 | Loss: 0.0566
2026-01-30 08:15:00 | INFO | Epoch 7 | Batch 100/1169 | Loss: 0.0525
2026-01-30 08:15:01 | INFO | Epoch 7 | Batch 200/1169 | Loss: 0.0550
2026-01-30 08:15:03 | INFO | Epoch 7 | Batch 300/1169 | Loss: 0.0570
2026-01-30 08:15:04 | INFO | Epoch 7 | Batch 400/1169 | Loss: 0.0549
2026-01-30 08:15:06 | INFO | Epoch 7 | Batch 500/1169 | Loss: 0.0568
2026-01-30 08:15:07 | INFO | Epoch 7 | Batch 600/1169 | Loss: 0.0528
2026-01-30 08:15:08 | INFO | Epoch 7 | Batch 700/1169 | Loss: 0.0543
2026-01-30 08:15:09 | INFO | Epoch 7 | Batch 800/1169 | Loss: 0.0576
2026-01-30 08:15:11 | INFO | Epoch 7 | Batch 900/1169 | Loss: 0.0572
2026-01-30 08:15:12 | INFO | Epoch 7 | Batch 1000/1169 | Loss: 0.0572
2026-01-30 08:15:14 | INFO | Epoch 7 | Batch 1100/1169 | Loss: 0.0581
2026-01-30 08:15:16 | INFO | Epoch 7 | Train Loss: 0.0558 | Val Loss: 0.0454
2026-01-30 08:15:16 | INFO | Val F1: 0.1365 | Prec: 0.0751 | Rec: 0.7452 | AUC: 0.7794
2026-01-30 08:15:16 | INFO | Epoch 8 | Batch 0/1169 | Loss: 0.0526
2026-01-30 08:15:17 | INFO | Epoch 8 | Batch 100/1169 | Loss: 0.0573
2026-01-30 08:15:19 | INFO | Epoch 8 | Batch 200/1169 | Loss: 0.0591
2026-01-30 08:15:20 | INFO | Epoch 8 | Batch 300/1169 | Loss: 0.0574
2026-01-30 08:15:22 | INFO | Epoch 8 | Batch 400/1169 | Loss: 0.0562
2026-01-30 08:15:23 | INFO | Epoch 8 | Batch 500/1169 | Loss: 0.0526
2026-01-30 08:15:24 | INFO | Epoch 8 | Batch 600/1169 | Loss: 0.0520
2026-01-30 08:15:26 | INFO | Epoch 8 | Batch 700/1169 | Loss: 0.0575
2026-01-30 08:15:27 | INFO | Epoch 8 | Batch 800/1169 | Loss: 0.0563
2026-01-30 08:15:29 | INFO | Epoch 8 | Batch 900/1169 | Loss: 0.0555
2026-01-30 08:15:30 | INFO | Epoch 8 | Batch 1000/1169 | Loss: 0.0560
2026-01-30 08:15:32 | INFO | Epoch 8 | Batch 1100/1169 | Loss: 0.0589
2026-01-30 08:15:34 | INFO | Epoch 8 | Train Loss: 0.0555 | Val Loss: 0.0432
2026-01-30 08:15:34 | INFO | Val F1: 0.1308 | Prec: 0.0715 | Rec: 0.7642 | AUC: 0.7809
2026-01-30 08:15:34 | INFO | Epoch 9 | Batch 0/1169 | Loss: 0.0558
2026-01-30 08:15:35 | INFO | Epoch 9 | Batch 100/1169 | Loss: 0.0566
2026-01-30 08:15:37 | INFO | Epoch 9 | Batch 200/1169 | Loss: 0.0512
2026-01-30 08:15:38 | INFO | Epoch 9 | Batch 300/1169 | Loss: 0.0579
2026-01-30 08:15:39 | INFO | Epoch 9 | Batch 400/1169 | Loss: 0.0585
2026-01-30 08:15:41 | INFO | Epoch 9 | Batch 500/1169 | Loss: 0.0555
2026-01-30 08:15:42 | INFO | Epoch 9 | Batch 600/1169 | Loss: 0.0585
2026-01-30 08:15:43 | INFO | Epoch 9 | Batch 700/1169 | Loss: 0.0544
2026-01-30 08:15:44 | INFO | Epoch 9 | Batch 800/1169 | Loss: 0.0587
2026-01-30 08:15:46 | INFO | Epoch 9 | Batch 900/1169 | Loss: 0.0541
2026-01-30 08:15:47 | INFO | Epoch 9 | Batch 1000/1169 | Loss: 0.0590
2026-01-30 08:15:48 | INFO | Epoch 9 | Batch 1100/1169 | Loss: 0.0520
2026-01-30 08:15:51 | INFO | Epoch 9 | Train Loss: 0.0552 | Val Loss: 0.0403
2026-01-30 08:15:51 | INFO | Val F1: 0.1547 | Prec: 0.0870 | Rec: 0.6990 | AUC: 0.7835
2026-01-30 08:15:51 | INFO | New best model saved (F1: 0.1547)
2026-01-30 08:15:51 | INFO | Epoch 10 | Batch 0/1169 | Loss: 0.0537
2026-01-30 08:15:52 | INFO | Epoch 10 | Batch 100/1169 | Loss: 0.0568
2026-01-30 08:15:54 | INFO | Epoch 10 | Batch 200/1169 | Loss: 0.0535
2026-01-30 08:15:56 | INFO | Epoch 10 | Batch 300/1169 | Loss: 0.0536
2026-01-30 08:15:57 | INFO | Epoch 10 | Batch 400/1169 | Loss: 0.0554
2026-01-30 08:15:58 | INFO | Epoch 10 | Batch 500/1169 | Loss: 0.0555
2026-01-30 08:16:00 | INFO | Epoch 10 | Batch 600/1169 | Loss: 0.0509
2026-01-30 08:16:01 | INFO | Epoch 10 | Batch 700/1169 | Loss: 0.0521
2026-01-30 08:16:03 | INFO | Epoch 10 | Batch 800/1169 | Loss: 0.0581
2026-01-30 08:16:04 | INFO | Epoch 10 | Batch 900/1169 | Loss: 0.0549
2026-01-30 08:16:05 | INFO | Epoch 10 | Batch 1000/1169 | Loss: 0.0565
2026-01-30 08:16:07 | INFO | Epoch 10 | Batch 1100/1169 | Loss: 0.0514
2026-01-30 08:16:09 | INFO | Epoch 10 | Train Loss: 0.0548 | Val Loss: 0.0411
2026-01-30 08:16:09 | INFO | Val F1: 0.1438 | Prec: 0.0796 | Rec: 0.7419 | AUC: 0.7826
2026-01-30 08:16:09 | INFO | Epoch 11 | Batch 0/1169 | Loss: 0.0552
2026-01-30 08:16:11 | INFO | Epoch 11 | Batch 100/1169 | Loss: 0.0537
2026-01-30 08:16:12 | INFO | Epoch 11 | Batch 200/1169 | Loss: 0.0551
2026-01-30 08:16:13 | INFO | Epoch 11 | Batch 300/1169 | Loss: 0.0549
2026-01-30 08:16:15 | INFO | Epoch 11 | Batch 400/1169 | Loss: 0.0521
2026-01-30 08:16:16 | INFO | Epoch 11 | Batch 500/1169 | Loss: 0.0522
2026-01-30 08:16:17 | INFO | Epoch 11 | Batch 600/1169 | Loss: 0.0569
2026-01-30 08:16:19 | INFO | Epoch 11 | Batch 700/1169 | Loss: 0.0515
2026-01-30 08:16:20 | INFO | Epoch 11 | Batch 800/1169 | Loss: 0.0577
2026-01-30 08:16:22 | INFO | Epoch 11 | Batch 900/1169 | Loss: 0.0605
2026-01-30 08:16:23 | INFO | Epoch 11 | Batch 1000/1169 | Loss: 0.0577
2026-01-30 08:16:24 | INFO | Epoch 11 | Batch 1100/1169 | Loss: 0.0518
2026-01-30 08:16:27 | INFO | Epoch 11 | Train Loss: 0.0546 | Val Loss: 0.0428
2026-01-30 08:16:27 | INFO | Val F1: 0.1430 | Prec: 0.0791 | Rec: 0.7390 | AUC: 0.7842
2026-01-30 08:16:27 | INFO | Epoch 12 | Batch 0/1169 | Loss: 0.0598
2026-01-30 08:16:28 | INFO | Epoch 12 | Batch 100/1169 | Loss: 0.0549
2026-01-30 08:16:29 | INFO | Epoch 12 | Batch 200/1169 | Loss: 0.0572
2026-01-30 08:16:31 | INFO | Epoch 12 | Batch 300/1169 | Loss: 0.0569
2026-01-30 08:16:32 | INFO | Epoch 12 | Batch 400/1169 | Loss: 0.0550
2026-01-30 08:16:34 | INFO | Epoch 12 | Batch 500/1169 | Loss: 0.0566
2026-01-30 08:16:35 | INFO | Epoch 12 | Batch 600/1169 | Loss: 0.0583
2026-01-30 08:16:37 | INFO | Epoch 12 | Batch 700/1169 | Loss: 0.0552
2026-01-30 08:16:38 | INFO | Epoch 12 | Batch 800/1169 | Loss: 0.0572
2026-01-30 08:16:39 | INFO | Epoch 12 | Batch 900/1169 | Loss: 0.0527
2026-01-30 08:16:41 | INFO | Epoch 12 | Batch 1000/1169 | Loss: 0.0558
2026-01-30 08:16:42 | INFO | Epoch 12 | Batch 1100/1169 | Loss: 0.0526
2026-01-30 08:16:44 | INFO | Epoch 12 | Train Loss: 0.0543 | Val Loss: 0.0419
2026-01-30 08:16:44 | INFO | Val F1: 0.1427 | Prec: 0.0788 | Rec: 0.7516 | AUC: 0.7859
2026-01-30 08:16:44 | INFO | Epoch 13 | Batch 0/1169 | Loss: 0.0546
2026-01-30 08:16:46 | INFO | Epoch 13 | Batch 100/1169 | Loss: 0.0557
2026-01-30 08:16:47 | INFO | Epoch 13 | Batch 200/1169 | Loss: 0.0529
2026-01-30 08:16:48 | INFO | Epoch 13 | Batch 300/1169 | Loss: 0.0545
2026-01-30 08:16:49 | INFO | Epoch 13 | Batch 400/1169 | Loss: 0.0505
2026-01-30 08:16:51 | INFO | Epoch 13 | Batch 500/1169 | Loss: 0.0544
2026-01-30 08:16:52 | INFO | Epoch 13 | Batch 600/1169 | Loss: 0.0554
2026-01-30 08:16:54 | INFO | Epoch 13 | Batch 700/1169 | Loss: 0.0570
2026-01-30 08:16:55 | INFO | Epoch 13 | Batch 800/1169 | Loss: 0.0535
2026-01-30 08:16:57 | INFO | Epoch 13 | Batch 900/1169 | Loss: 0.0523
2026-01-30 08:16:58 | INFO | Epoch 13 | Batch 1000/1169 | Loss: 0.0535
2026-01-30 08:16:59 | INFO | Epoch 13 | Batch 1100/1169 | Loss: 0.0574
2026-01-30 08:17:02 | INFO | Epoch 13 | Train Loss: 0.0542 | Val Loss: 0.0428
2026-01-30 08:17:02 | INFO | Val F1: 0.1447 | Prec: 0.0802 | Rec: 0.7390 | AUC: 0.7863
2026-01-30 08:17:02 | INFO | Epoch 14 | Batch 0/1169 | Loss: 0.0545
2026-01-30 08:17:04 | INFO | Epoch 14 | Batch 100/1169 | Loss: 0.0568
2026-01-30 08:17:05 | INFO | Epoch 14 | Batch 200/1169 | Loss: 0.0560
2026-01-30 08:17:06 | INFO | Epoch 14 | Batch 300/1169 | Loss: 0.0534
2026-01-30 08:17:08 | INFO | Epoch 14 | Batch 400/1169 | Loss: 0.0522
2026-01-30 08:17:09 | INFO | Epoch 14 | Batch 500/1169 | Loss: 0.0516
2026-01-30 08:17:11 | INFO | Epoch 14 | Batch 600/1169 | Loss: 0.0507
2026-01-30 08:17:12 | INFO | Epoch 14 | Batch 700/1169 | Loss: 0.0543
2026-01-30 08:17:13 | INFO | Epoch 14 | Batch 800/1169 | Loss: 0.0517
2026-01-30 08:17:15 | INFO | Epoch 14 | Batch 900/1169 | Loss: 0.0534
2026-01-30 08:17:16 | INFO | Epoch 14 | Batch 1000/1169 | Loss: 0.0541
2026-01-30 08:17:17 | INFO | Epoch 14 | Batch 1100/1169 | Loss: 0.0530
2026-01-30 08:17:20 | INFO | Epoch 14 | Train Loss: 0.0539 | Val Loss: 0.0426
2026-01-30 08:17:20 | INFO | Val F1: 0.1387 | Prec: 0.0764 | Rec: 0.7477 | AUC: 0.7865
2026-01-30 08:17:20 | INFO | Epoch 15 | Batch 0/1169 | Loss: 0.0556
2026-01-30 08:17:21 | INFO | Epoch 15 | Batch 100/1169 | Loss: 0.0588
2026-01-30 08:17:22 | INFO | Epoch 15 | Batch 200/1169 | Loss: 0.0521
2026-01-30 08:17:24 | INFO | Epoch 15 | Batch 300/1169 | Loss: 0.0514
2026-01-30 08:17:25 | INFO | Epoch 15 | Batch 400/1169 | Loss: 0.0522
2026-01-30 08:17:27 | INFO | Epoch 15 | Batch 500/1169 | Loss: 0.0518
2026-01-30 08:17:28 | INFO | Epoch 15 | Batch 600/1169 | Loss: 0.0518
2026-01-30 08:17:29 | INFO | Epoch 15 | Batch 700/1169 | Loss: 0.0518
2026-01-30 08:17:31 | INFO | Epoch 15 | Batch 800/1169 | Loss: 0.0560
2026-01-30 08:17:32 | INFO | Epoch 15 | Batch 900/1169 | Loss: 0.0556
2026-01-30 08:17:34 | INFO | Epoch 15 | Batch 1000/1169 | Loss: 0.0558
2026-01-30 08:17:35 | INFO | Epoch 15 | Batch 1100/1169 | Loss: 0.0489
2026-01-30 08:17:38 | INFO | Epoch 15 | Train Loss: 0.0537 | Val Loss: 0.0397
2026-01-30 08:17:38 | INFO | Val F1: 0.1502 | Prec: 0.0838 | Rec: 0.7226 | AUC: 0.7880
2026-01-30 08:17:38 | INFO | Epoch 16 | Batch 0/1169 | Loss: 0.0540
2026-01-30 08:17:39 | INFO | Epoch 16 | Batch 100/1169 | Loss: 0.0517
2026-01-30 08:17:41 | INFO | Epoch 16 | Batch 200/1169 | Loss: 0.0544
2026-01-30 08:17:42 | INFO | Epoch 16 | Batch 300/1169 | Loss: 0.0592
2026-01-30 08:17:43 | INFO | Epoch 16 | Batch 400/1169 | Loss: 0.0568
2026-01-30 08:17:44 | INFO | Epoch 16 | Batch 500/1169 | Loss: 0.0516
2026-01-30 08:17:46 | INFO | Epoch 16 | Batch 600/1169 | Loss: 0.0569
2026-01-30 08:17:47 | INFO | Epoch 16 | Batch 700/1169 | Loss: 0.0525
2026-01-30 08:17:49 | INFO | Epoch 16 | Batch 800/1169 | Loss: 0.0560
2026-01-30 08:17:50 | INFO | Epoch 16 | Batch 900/1169 | Loss: 0.0535
2026-01-30 08:17:51 | INFO | Epoch 16 | Batch 1000/1169 | Loss: 0.0496
2026-01-30 08:17:53 | INFO | Epoch 16 | Batch 1100/1169 | Loss: 0.0538
2026-01-30 08:17:55 | INFO | Epoch 16 | Train Loss: 0.0536 | Val Loss: 0.0420
2026-01-30 08:17:55 | INFO | Val F1: 0.1445 | Prec: 0.0800 | Rec: 0.7458 | AUC: 0.7869
2026-01-30 08:17:55 | INFO | Epoch 17 | Batch 0/1169 | Loss: 0.0580
2026-01-30 08:17:57 | INFO | Epoch 17 | Batch 100/1169 | Loss: 0.0497
2026-01-30 08:17:58 | INFO | Epoch 17 | Batch 200/1169 | Loss: 0.0522
2026-01-30 08:18:00 | INFO | Epoch 17 | Batch 300/1169 | Loss: 0.0549
2026-01-30 08:18:01 | INFO | Epoch 17 | Batch 400/1169 | Loss: 0.0501
2026-01-30 08:18:02 | INFO | Epoch 17 | Batch 500/1169 | Loss: 0.0548
2026-01-30 08:18:04 | INFO | Epoch 17 | Batch 600/1169 | Loss: 0.0537
2026-01-30 08:18:05 | INFO | Epoch 17 | Batch 700/1169 | Loss: 0.0526
2026-01-30 08:18:06 | INFO | Epoch 17 | Batch 800/1169 | Loss: 0.0577
2026-01-30 08:18:08 | INFO | Epoch 17 | Batch 900/1169 | Loss: 0.0542
2026-01-30 08:18:09 | INFO | Epoch 17 | Batch 1000/1169 | Loss: 0.0502
2026-01-30 08:18:10 | INFO | Epoch 17 | Batch 1100/1169 | Loss: 0.0554
2026-01-30 08:18:13 | INFO | Epoch 17 | Train Loss: 0.0534 | Val Loss: 0.0406
2026-01-30 08:18:13 | INFO | Val F1: 0.1425 | Prec: 0.0787 | Rec: 0.7497 | AUC: 0.7871
2026-01-30 08:18:13 | INFO | Epoch 18 | Batch 0/1169 | Loss: 0.0555
2026-01-30 08:18:14 | INFO | Epoch 18 | Batch 100/1169 | Loss: 0.0558
2026-01-30 08:18:15 | INFO | Epoch 18 | Batch 200/1169 | Loss: 0.0543
2026-01-30 08:18:17 | INFO | Epoch 18 | Batch 300/1169 | Loss: 0.0523
2026-01-30 08:18:18 | INFO | Epoch 18 | Batch 400/1169 | Loss: 0.0562
2026-01-30 08:18:19 | INFO | Epoch 18 | Batch 500/1169 | Loss: 0.0530
2026-01-30 08:18:21 | INFO | Epoch 18 | Batch 600/1169 | Loss: 0.0575
2026-01-30 08:18:22 | INFO | Epoch 18 | Batch 700/1169 | Loss: 0.0506
2026-01-30 08:18:23 | INFO | Epoch 18 | Batch 800/1169 | Loss: 0.0546
2026-01-30 08:18:24 | INFO | Epoch 18 | Batch 900/1169 | Loss: 0.0540
2026-01-30 08:18:26 | INFO | Epoch 18 | Batch 1000/1169 | Loss: 0.0512
2026-01-30 08:18:27 | INFO | Epoch 18 | Batch 1100/1169 | Loss: 0.0490
2026-01-30 08:18:30 | INFO | Epoch 18 | Train Loss: 0.0533 | Val Loss: 0.0416
2026-01-30 08:18:30 | INFO | Val F1: 0.1368 | Prec: 0.0751 | Rec: 0.7642 | AUC: 0.7880
2026-01-30 08:18:30 | INFO | Epoch 19 | Batch 0/1169 | Loss: 0.0486
2026-01-30 08:18:31 | INFO | Epoch 19 | Batch 100/1169 | Loss: 0.0524
2026-01-30 08:18:33 | INFO | Epoch 19 | Batch 200/1169 | Loss: 0.0573
2026-01-30 08:18:34 | INFO | Epoch 19 | Batch 300/1169 | Loss: 0.0501
2026-01-30 08:18:36 | INFO | Epoch 19 | Batch 400/1169 | Loss: 0.0511
2026-01-30 08:18:37 | INFO | Epoch 19 | Batch 500/1169 | Loss: 0.0536
2026-01-30 08:18:39 | INFO | Epoch 19 | Batch 600/1169 | Loss: 0.0558
2026-01-30 08:18:40 | INFO | Epoch 19 | Batch 700/1169 | Loss: 0.0512
2026-01-30 08:18:42 | INFO | Epoch 19 | Batch 800/1169 | Loss: 0.0545
2026-01-30 08:18:43 | INFO | Epoch 19 | Batch 900/1169 | Loss: 0.0530
2026-01-30 08:18:44 | INFO | Epoch 19 | Batch 1000/1169 | Loss: 0.0497
2026-01-30 08:18:45 | INFO | Epoch 19 | Batch 1100/1169 | Loss: 0.0530
2026-01-30 08:18:48 | INFO | Epoch 19 | Train Loss: 0.0531 | Val Loss: 0.0419
2026-01-30 08:18:48 | INFO | Val F1: 0.1516 | Prec: 0.0848 | Rec: 0.7177 | AUC: 0.7881
2026-01-30 08:18:48 | INFO | Early stopping at epoch 19
2026-01-30 08:18:49 | INFO | ==================================================
2026-01-30 08:18:49 | INFO | TEST RESULTS (GNN Model)
2026-01-30 08:18:49 | INFO | ==================================================
2026-01-30 08:18:49 | INFO | F1: 0.1541
2026-01-30 08:18:49 | INFO | Precision: 0.0867
2026-01-30 08:18:49 | INFO | Recall: 0.6909
2026-01-30 08:18:49 | INFO | AUC: 0.7770
